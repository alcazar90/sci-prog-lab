{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVHYySrrXafH"
      },
      "source": [
        "<h1><center>Laboratorio 9: Los huesos de Hipócrates 🦴</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHPut9OIXafL"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesor: Matías Rojas y Mauricio Araneda\n",
        "- Auxiliar: Ignacio Meza D.\n",
        "- Ayudante: Rodrigo Guerra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDPGaSh3XafM"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n",
        "\n",
        "- Nombre de alumno 1: Cristóbal Alcázar\n",
        "- Nombre de alumno 2: Gianina Salomó\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibpy_PFlXafM"
      },
      "source": [
        "### **Link de repositorio de GitHub:** `https://github.com/alcazar90/sci-prog-lab`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZv01tEdXafN"
      },
      "source": [
        "### Indice \n",
        "\n",
        "1. [Temas a tratar](#Temas-a-tratar:)\n",
        "3. [Descripcción del laboratorio](#Descripción-del-laboratorio.)\n",
        "4. [Desarrollo](#Desarrollo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cayY65DnXafN"
      },
      "source": [
        "## Temas a tratar\n",
        "\n",
        "- Creación de clasificadores de imagenes a traves de redes Fully connected y CNN.\n",
        "- Uso de Dataloaders para la carga de datasets.\n",
        "- Comparación de Fully Connected y red convolucional.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- Fecha de entrega: 17/11/2022\n",
        "- **Grupos de 2 personas**\n",
        "- **Ausentes** deberán realizar la actividad solos. \n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Prohibidas las copias. \n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Creación de modelos de clasificación de imágenes utilizando Pytorch.\n",
        "- Creación de dataloader y aplicar transformaciones sobre el dataset.\n",
        "- Comprender la diferencia entre una CNN y una Fully Connected.\n",
        "\n",
        "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones de `Pytorch`, la cual, está enfocada para proyectos de Deep Learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTodVg3qXafO"
      },
      "source": [
        "# Importamos librerias utiles 😸\n",
        "\n",
        "Comenzamos importando librerías utiles para la ejecución del laboratorio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SPKUy4UKXafP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models\n",
        "from torchvision import transforms as T\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVNCQAQNdVRH",
        "outputId": "9f29956e-d872-4d0d-e568-53752dc3a13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlk9g2mgXafQ"
      },
      "source": [
        "# Identificando los Huesos de Hipócrates🔎\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/d8/58/66/d85866cd1cc3979f94526551addf74b4.gif\" width=\"300\">\n",
        "</p>\n",
        "\n",
        "Tras el éxito que han tenido proponiendo modelos de machine learning en trabajos anteriores, el famoso medico Hipócrates se ha contactado con ustedes para solicitarles ayuda para automatizar la identificación de radiografías de partes humanas. Para esto, les señala que le gustaría utilizar algoritmos de deep learning producto que Demócrito le señalo que resultan la mejor alternativa para la predicción de imágenes.\n",
        "\n",
        "En su conversación con el medico usted le comenta que ha tenido algunas clases relacionadas a Deep Learning, por esto, están motivados en abordar el problema utilizando redes Fully Connected y redes convolucionales con Pytorch. Sin embargo, al anunciarle los tipos de redes que conocen, el filósofo les comenta que no había escuchado muy buenos resultados por parte de las CNN, por lo que les pide que comprueben a traves de la métrica de accuracy que tipo de redes es mejor para la tarea de identificación de radiografías. ¿Será cierto lo que dice el filósofo?, Veámoslo en un nuevo capítulo de los Laboratorios de Programación Científica para Ciencia de Datos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-HCdzXCXafR"
      },
      "source": [
        "## 1.1 Creación de Lista de Archivos\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/BJ-9w-MUVCMAAAAM/tis100-sad.gif\" width=\"300\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N4BiamZXafR"
      },
      "source": [
        "Comience revisando de forma manual cada una de las imágenes que posee la carpeta subida a material docente. Verifique la cantidad de tipos de radiografías que se tienen y la cantidad de imágenes que dispone cada carpeta.\n",
        "\n",
        "Revisado el contenido de las imágenes, utilice `os.listdir` para crear un `numpy.array` o un `Dataframe` que contenga las imágenes y un label que señale al tipo de radiografía a la que hace referencia la imagen. Para hacer las etiquetas codifique el tipo de imágenes en números que vayan del 0 al total de tipos de radiografías, no utilice strings para codificar las etiquetas.\n",
        "\n",
        "**Ejemplo de Estructura:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hDV1stVXafS"
      },
      "source": [
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
        "</style>\n",
        "<table class=\"tg\">\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th class=\"tg-0lax\"></th>\n",
        "    <th class=\"tg-0lax\">image_path</th>\n",
        "    <th class=\"tg-0lax\">label</th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">0</td>\n",
        "    <td class=\"tg-0lax\">image1</td>\n",
        "    <td class=\"tg-0lax\">1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">1</td>\n",
        "    <td class=\"tg-0lax\">image2</td>\n",
        "    <td class=\"tg-0lax\">0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">2</td>\n",
        "    <td class=\"tg-0lax\">image3</td>\n",
        "    <td class=\"tg-0lax\">2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">3</td>\n",
        "    <td class=\"tg-0lax\">image4</td>\n",
        "    <td class=\"tg-0lax\">0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">4</td>\n",
        "    <td class=\"tg-0lax\">image5</td>\n",
        "    <td class=\"tg-0lax\">4</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wjs3VE4XafS"
      },
      "outputs": [],
      "source": [
        "# Código Aquí\n",
        "#for i, category in enumerate(os.listdir(\"ruta de carpeta\")):\n",
        "#    for image i n os.listdir(\"ruta de subcarpetas\"):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip gdrive/My\\ Drive/Medical-MNIST.zip"
      ],
      "metadata": {
        "id": "PMeSo2L0djiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_df = pd.DataFrame([\n",
        "    {\n",
        "        \"image_path\": \"/\".join(f.split(\"/\")[1:]),\n",
        "        \"label\": f.split(\"/\")[1]\n",
        "    }\n",
        "    for f in glob(\"Medical-MNIST/*/*.jpeg\")\n",
        "])\n",
        "\n",
        "path_df[\"label\"] = path_df[\"label\"].replace(\n",
        "    sorted(path_df[\"label\"].unique()),\n",
        "    range(path_df[\"label\"].nunique())\n",
        ")\n",
        "\n",
        "path_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gLt8vEF1bDjB",
        "outputId": "c9491f54-ef8f-429c-b79a-2de89e3e2c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         image_path  label\n",
              "0  Hand/008745.jpeg      4\n",
              "1  Hand/007006.jpeg      4\n",
              "2  Hand/004826.jpeg      4\n",
              "3  Hand/002630.jpeg      4\n",
              "4  Hand/009559.jpeg      4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51169d7c-528c-4f29-8008-0314f7427859\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hand/008745.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hand/007006.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hand/004826.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hand/002630.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hand/009559.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51169d7c-528c-4f29-8008-0314f7427859')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51169d7c-528c-4f29-8008-0314f7427859 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51169d7c-528c-4f29-8008-0314f7427859');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_df[\"label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Br7AHMEklO3",
        "outputId": "edace28b-663e-42b1-e855-bc4973fe8875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    10000\n",
              "3    10000\n",
              "2    10000\n",
              "0    10000\n",
              "5    10000\n",
              "1     8954\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Las clases se encuentran balanceadas"
      ],
      "metadata": {
        "id": "iDO8Hx9Ukozl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrftPjphXafT"
      },
      "source": [
        "## 1.2 Creación de Dataset\n",
        "\n",
        "Tomando en cuenta la estructura de datos desarrollada en el punto 1.1, construya la clase `MedicalDataset()` que cumpla los siguientes puntos:\n",
        "\n",
        "- [X] Poseer un `__init__` en el que se almacene `estructura` creada en 1.1, la `raiz` de la carpeta y una función que permita transformar el dataset (de esto no se preocupe mucho, ya que solamente debe almacenar una función en el atributo).\n",
        "- [X] La clase debe ser capaz de entregar la cantidad de elementos a traves de `__len__`.\n",
        "- [X] Debe poseer el método `__getitem__` que retorne una tupla con la imagen y su correspondiente etiqueta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjYcR-V4XafT"
      },
      "outputs": [],
      "source": [
        "# Código Aquí\n",
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, estructura, raiz, transform=False):\n",
        "        self.estructura = estructura\n",
        "        self.raiz = raiz\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # Un poco de ayuda para cargar la imagen\n",
        "        img_path = os.path.join(self.raiz, self.estructura[\"image_path\"][idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.estructura[\"label\"][idx]\n",
        "        \n",
        "        # Auida para realizar la transformación\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.estructura.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "henHo3G7XafT"
      },
      "source": [
        "## 1.3 Prueba del MedicalDataset\n",
        "\n",
        "Con la clase construida en el punto 1.2, verifique su funcionamiento cargando el dataset y realizando las transformaciones que entrega la función `transform_image`. Compruebe a través de un ejemplo las transformaciones aplicadas en la imagen, comentando la función que cumple `MedicalDataset` y si es posible observar todas las transformaciones aplicadas con la función `transform_image`.\n",
        "\n",
        "- [X] Probar la clase MedicalDataset y aplicando una transformación de \"train\"\n",
        "- [X] Plotear un ejemplo del MedicalDataset.\n",
        "\n",
        "**Función para transformar las imagenes:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eAV6styXafU"
      },
      "outputs": [],
      "source": [
        "def transform_image(stage = None):\n",
        "    \n",
        "    if stage == \"train\":\n",
        "        Tr_img = T.Compose([\n",
        "                  T.Resize(size = (64,64)),\n",
        "                  #T.Resize(size = (256,256)),\n",
        "                  T.RandomRotation(degrees = (-20,+20)),\n",
        "                  T.ToTensor()])\n",
        "        \n",
        "    elif stage == \"test\" or stage == \"val\":\n",
        "        Tr_img = T.Compose([\n",
        "                  T.Resize(size = (64,64)), \n",
        "                  T.ToTensor()]) \n",
        "\n",
        "    return Tr_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBVMvOlXafU"
      },
      "source": [
        "**Código para obtener un ejemplo:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0bNfbp5XafU"
      },
      "outputs": [],
      "source": [
        "# Prueba del dataset\n",
        "dataset = MedicalDataset(path_df, \"Medical-MNIST\", transform_image(\"train\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_image, example_label = dataset[0]\n",
        "# Utilice plotly para plotear un ejemplo\n",
        "px.imshow(example_image.numpy().T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "SLwYV8LaReb6",
        "outputId": "0b063b78-6b7d-41fe-d32f-2b993333656c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"373acb1f-d8a8-41d9-8171-61153328656c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"373acb1f-d8a8-41d9-8171-61153328656c\")) {                    Plotly.newPlot(                        \"373acb1f-d8a8-41d9-8171-61153328656c\",                        [{\"name\":\"0\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<extra></extra>\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('373acb1f-d8a8-41d9-8171-61153328656c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imagen sin transformación\n",
        "dataset_sint = MedicalDataset(path_df, \"Medical-MNIST\")\n",
        "example_image_sint, example_label_sint = dataset_sint[0]\n",
        "px.imshow(example_image_sint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "a8f9BPtiSwQG",
        "outputId": "7efe0228-408f-44d4-83fd-8b17d5be2edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"9099af38-e9f0-4482-82f0-11369b71d447\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9099af38-e9f0-4482-82f0-11369b71d447\")) {                    Plotly.newPlot(                        \"9099af38-e9f0-4482-82f0-11369b71d447\",                        [{\"name\":\"0\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: [%{z[0]}, %{z[1]}, %{z[2]}]<extra></extra>\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9099af38-e9f0-4482-82f0-11369b71d447');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCmn_aWLXafW"
      },
      "source": [
        "> Comente que realiza la clase construida y las transformaciones aplicadas.\n",
        "\n",
        "La clase `MedicalDataset` permite construir el _dataset_, en base a imágenes que se encuentran organizadas en una estructura de carpetas (_aka ImageFolder_), y cuyas etiquetas son los nombres\n",
        "de los directorios. El funcionamiento de la clase es navegar\n",
        "por esta estructura y cargar la información de las\n",
        "imágenes de manera organizada y con sus etiquetas respectivas:\n",
        "\n",
        "* Una observación en el _dataset_ se constituye por una imagen\n",
        "representada como un `array`, o un `Tensor`, y una etiqueta representada por un entero. Por lo que una observación se\n",
        "conforma por la siguiente tupla: `(imagen, etiqueta)`.\n",
        "* La clase requiere un _dataframe_ (`path_df`) con dos columnas, donde la\n",
        "primera tiene el nombre de la imagen y la segunda su etiqueta. Es\n",
        "decir, una tabla con el inventario de todas las observaciones del _dataset_.\n",
        "* En base la información anterior, específicamente la primera \n",
        "columna, la clase `MedicalDataset` tiene la capacidad de\n",
        "reconstruir el `path` donde se encuentra la imagen. Esto en conjunto con el atributo `self.raiz`, que indica la ubicación del directorio donde se encuentra la estructura de carpetas con las imágenes, y así luego importar la imagen en un arreglo `numpy` usando `Image.open(img_path).convert('RGB')`.\n",
        "* La funcionalidad anterior es realizada por el método `MedicalDataset.__getitem__()`. Lo que permite acceder por índices\n",
        "a las observaciones del _dataset_. Por ejemplo,  `image_0, label_0 = MedicalDataset(..)[0]` nos entrega la tupla correspondiente\n",
        "a la primera observación.\n",
        "* Un punto a destacar de lo anterior, la clase `MedicalDataset` tiene la capacidad de cargar las observaciones que uno requiere, pero en ningún caso, como se puede apreciar en el código, importa todas las imágenes cuando se inicializa. Esto no es un error, por el tamaño de las imágenes, es más ventajoso que esta clase funcione\n",
        "como un rutero con capacidad de catrastrar la información y cargarla cuando se requiere para no colapsar la memoría. \n",
        "* Luego veremos que hay una especie de orquestado que aprovecha\n",
        "la funcionalidad de `MedicalDataset` para ir solicitando\n",
        "observaciones a medida que las necesita: `DataLoader`.\n",
        "* Es posible saber la cantidad de observaciones del _dataset_ utilizando el método `MedicalDataset.__len__(self)`, que inspecciona la cantidad de filas del\n",
        "_dataframe_ inventario de todas las observaciones.\n",
        "* Además, es posible posterior a importar una imagen, aplicar\n",
        "un conjunto de transformaciones como rotaciones, resize, entre \n",
        "otras. Esto se realiza con el atributo `self.transform`.\n",
        "\n",
        "**Respecto a las transformaciones:**\n",
        "\n",
        "* Las transformaciones aplicadas consisten en realizar una rotación de 20 grados de la imagen original, y aplicar un redimensionamiento a 64x64 pixeles.Además,  se cambia la estructura del tipo de dato de la imagen almacenada originalmente en numpy, a tipo de dato Tensor.\n",
        "\n",
        "* En este caso particular no cambia el tamaño de la imagen, ya que todas las imágenes son originalmente de 64x64, pero se debe tener en cuenta esta transformación en caso de que la red se entrene con otro conjunto de datos, o que se evalúe con imágenes de otro tamaño."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WARo1zQBXafW"
      },
      "source": [
        "## 1.4 Creación de Clasificadores\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/01/conv-full-layer.gif\" width=\"300\">\n",
        "</p>\n",
        "\n",
        "A continuación, deben construir tres clasificadores con los que deberán verificar cuál de las arquitecturas posee un mejor desempeño para la tarea de clasificación de imágenes. Para la construcción considere los siguientes puntos:\n",
        "\n",
        "- [X] Señale cual es el objetivo del `forward` en este tipo de redes, sea breve para su explicación.\n",
        "- [X] Construir una red Fully Connected para solucionar el problema de clasificación. Para esta parte se le aconseja que rellene el esqueleto dispuesto más abajo y que lleva el nombre de `FCClassifier`, en el deberá rellenar con la dimensión de las capas ocultas y verificar cual será el tamaño de la entrada.\n",
        "- [X] Construya una red convolucional **simple** (no más de una capa convolucional) para la tarea de clasificación de imágenes, para esto basen su código en la clase del día `09-11-2022`.\n",
        "- [X] Crear una red convolucional más compleja. Para esta parte tienen completa libertad en la construcción de su red, lo único que debe cumplir es que sea convolucional.\n",
        "\n",
        "**Esqueletos Propuestos:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo del método `forward`**: El\n",
        "_forward_ pass son todas las operaciones\n",
        "que se aplican al _input_ para llegar al output, o predicción. Corresponde a cómo la información del _input_ fluye a través de las capas que conforman la arquitectura de la red, aplicandose las operaciones lineales usando los parámetros (iniciados generalmente de forma aleatoria cuando se crea la red), funciones de activación no lineales, y otros mecanismos que se utilizan cuando se computan las predicciones durante entrenamiento (i.e. capas de dropout). Su retorno incluye de forma general la salida de la red para una entrada dada, que puede haber pasado o no por una función de activación de la capa de salida, dependiendo de la implementación."
      ],
      "metadata": {
        "id": "EurknL02mIYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbl06W5jXafW"
      },
      "outputs": [],
      "source": [
        "# Construir una red Fully Connected para solucionar el problema de clasificación.\n",
        "# Para esta parte se le aconseja que rellene el esqueleto dispuesto más abajo\n",
        "# y que lleva el nombre de FCClassifier...\n",
        "class FCClassifier(nn.Module):\n",
        "    # ... en él deberá rellenar con la dimensión de las capas ocultas...:\n",
        "    # R: Para ello agregamos un nuevo parámetro de lista que contiene\n",
        "    # la cantidad de neuronas de cada capa oculta.\n",
        "    def __init__(self, in_channels: int, hidden_sizes: list, num_classes: int):\n",
        "        super(FCClassifier, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # ...y verificar cual será el tamaño de la entrada.\n",
        "        assert isinstance(in_channels, int) and in_channels > 0\n",
        "\n",
        "        # Se asigna entrada a primera capa oculta\n",
        "        self.lin1 =  nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_sizes[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Se genera las siguientes capas ocultas\n",
        "        if len(hidden_sizes) > 1:\n",
        "          self.other_lin = torch.nn.ModuleList([\n",
        "              nn.Sequential(\n",
        "                  nn.Linear(hidden_sizes[i], s),\n",
        "                  nn.ReLU()\n",
        "              )\n",
        "              for i, s in enumerate(hidden_sizes[1:])         \n",
        "          ])\n",
        "\n",
        "        else:\n",
        "          self.other_lin = torch.nn.ModuleList([])\n",
        "\n",
        "        # Se cambia nombre a capa de salida\n",
        "        self.lin_out = nn.Linear(hidden_sizes[-1], num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.lin1(self.flatten(x))\n",
        "\n",
        "        # Otras capas ocultas\n",
        "        for lin in self.other_lin:\n",
        "          out = lin(x)\n",
        "\n",
        "        # Capa de salida (no se aplica activación)\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imvkOHgwXafW"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier1(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, num_classes):\n",
        "      super().__init__()\n",
        "      self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                                  padding=1)\n",
        "      # kernel_size=3 + padding=1 -> conserva las dimensiones de la imagen\n",
        "      # original luego de aplicar la capa convolucional\n",
        "      self.fc = nn.Linear(64 * 64 * out_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = F.relu(self.conv_layer(x))\n",
        "      out = torch.flatten(out, 1)\n",
        "      out = self.fc(out)\n",
        "      return out\n",
        "\n",
        "    \n",
        "class CNNClassifier2(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, mid_channels, num_classes):\n",
        "      super().__init__()\n",
        "      # Combinación kernel_size=3 y padding=1 permiten conservar\n",
        "      # las dimensiones del input una vez aplicada la capa convolucional\n",
        "      # Elección: facilita pasar luego a la capa lineal debido a que no\n",
        "      # hay que llevar un \"conteo\" de la pérdida de pixeles cada vez que\n",
        "      # se aplica un kernel en una capa convolucional. Razón práctica.\n",
        "      self.conv_layer1 = nn.Conv2d(in_channels, mid_channels[0], kernel_size=3,\n",
        "                                   padding=1)\n",
        "\n",
        "      # Se genera los siguientes bloques convolcuionales intermedios \n",
        "      if len(mid_channels) > 1:\n",
        "        self.other_conv = torch.nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.MaxPool2d(2, 2),\n",
        "                nn.Conv2d(mid_channels[i], mid_channels[i+1], kernel_size=3,\n",
        "                          padding=1),\n",
        "                nn.ReLU(),\n",
        "                # La introducción de más bloques puede provocar overfitting\n",
        "                # para alivianar este problema se agrega una capa de dropout\n",
        "                # como regularización (50% de las neuronas se apaga -> p=0.5)\n",
        "                nn.Dropout(p=0.5)\n",
        "          )\n",
        "            for i, s in enumerate(mid_channels[:-1])         \n",
        "        ])\n",
        "        # Factor que lleva la cuenta de cuantas veces las dimensiones\n",
        "        # de la imagen inicia HxW son reducidas a la mitad por la capa de\n",
        "        # nn.MaxPool2d(2,2)\n",
        "        self.shrink_factor = 2 ** (len(mid_channels)-1)\n",
        "\n",
        "      else:\n",
        "        self.other_conv = torch.nn.ModuleList([])\n",
        "        # Si no hay capas convolucionales, el factor es 1 (no se reduce)\n",
        "        self.shrink_factor = 2 ** 0\n",
        "\n",
        "      self.fc = nn.Linear(int(64 / self.shrink_factor) * int(64 / self.shrink_factor) * mid_channels[-1], num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = F.relu(self.conv_layer1(x))\n",
        "\n",
        "      # Aplicar bloques convolucionales intermedios: conv - act - pool\n",
        "      for conv in self.other_conv:\n",
        "        out = conv(out)\n",
        "\n",
        "      # Aplanar el vector\n",
        "      out = torch.flatten(out, 1)\n",
        "\n",
        "      # Apicar capa fully connected\n",
        "      out = self.fc(out)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos un _sanity check_ con las arquitecturas convolucionales \n",
        "para verificar si el diseño de las capas están correctamente\n",
        "especificados según las dimensiones del _input_.\n",
        "\n",
        "Para esto, utilizamos una solo imagen a color de ejemplo (`3x64x64`), y agregamos una dimensión adicional que indica\n",
        "el tamaño del _batch_ (`1x3x64x64`). Esto lo podemos\n",
        "hacer facilmente usando el método `.unsqueeze(0)` para agregar\n",
        "una dimensión a un tensor en la posición 0 (aka primera). Luego, probamos\n",
        "si la información fluye corrrectamente en el _forward pass_:"
      ],
      "metadata": {
        "id": "HGouKPPhQOnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Si no arroja error significa que la arquitectura responde a las\n",
        "# dimensiones del dataset\n",
        "torch.manual_seed(666)\n",
        "\n",
        "cnn = CNNClassifier1(in_channels=3, out_channels=10, num_classes=4)\n",
        "cnn(dataset[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHPwYVY3Oe6H",
        "outputId": "af9261e7-023d-4601-8b01-1f95921b1ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0212,  0.0151, -0.1896, -0.0924]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, probamos la red convolucional más compleja que\n",
        "permite agregar bloques convoluciones intermedio, cada bloque\n",
        "esta compuesto por:\n",
        "\n",
        "> `nn.MaxPool2d(2,2)` -> `nn.Conv2d(mid_channels[i], mid_channels[i+1])` -> `nn.ReLU() `\n",
        "\n",
        "Notemos que siempre reducimos el tamaño de la imagen sobre\n",
        "la que se esta aplicando el kernel a la mitad (i.e. 64 - 32 - 16),\n",
        "tantos bloques apliquemos (i.e. `len(mid_channels`). Los parámetros de cada capa convolucional se encuentran especificados\n",
        "en los elementos de `mid_channels`.\n",
        "\n",
        "Ahora probemos el funcionamiento con un `len(mid_channels)=1` y\n",
        "cuyo tamaño de canal es `10`. Esto debiera ser equivalente a la\n",
        "primera arquitectura convolucional con `out_channels=10`. Donde\n",
        "técnicamente no utilizamos ningún bloque convolucional."
      ],
      "metadata": {
        "id": "rwu_iMwrWvcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(666)\n",
        "\n",
        "cnn2 = CNNClassifier2(in_channels=3, mid_channels=[10], num_classes=4)\n",
        "cnn2(dataset[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMvM2fyLVz7u",
        "outputId": "cc5539ea-df8b-48c5-b4bb-f9943fce526b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0212,  0.0151, -0.1896, -0.0924]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los resultados son iguales.\n",
        "\n",
        "Ahora probemos una red más compleja, con 4 bloques \n",
        "convolucionales: `mid_channels=[10, 50, 75, 50]`."
      ],
      "metadata": {
        "id": "HFURBH0_ZofU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(666)\n",
        "\n",
        "cnn2 = CNNClassifier2(in_channels=3, mid_channels=[10, 50, 75, 50], num_classes=4)\n",
        "cnn2(dataset[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDNneLcjZwfi",
        "outputId": "b6d81169-74be-47b8-94c8-b6a544064ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0573,  0.0896,  0.0545, -0.0064]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ambas implementaciones de redes convolucionales están\n",
        "funcionando correctamente!\n"
      ],
      "metadata": {
        "id": "s5NHBJM8fCB7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxnVPPB6XafX"
      },
      "source": [
        "## 1.5 Separando Datos para el Entrenamiento\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://c.tenor.com/Esn7Jif-ZLQAAAAC/separate-square.gif\" width=\"200\">\n",
        "</p>\n",
        "\n",
        "Utilizando un Holdout a su gusto, separe los datos en un conjunto de entrenamiento y de testing. Aplique las transformaciones correspondientes usando `transform_image` para cada conjunto de datos y utilice `torch.utils.data.DataLoader` para crear un objeto iterable del dataset.\n",
        "\n",
        "- [X] Definir el Holdout a utilizar.\n",
        "- [X] Separar los datos en un conjunto de entrenamiento y prueba.\n",
        "- [X] Aplicar las transformaciones correspondientes en cada uno de los dataset.\n",
        "- [X] Utilizar `DataLoader` de pytorch sobre los dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definir holdout a utilizar**\n",
        "\n",
        "Se crean _dataframes_ con inventarios de observaciones para el conjunto de entrenamiento y prueba siguiendo una proporción de 80-20 respectivamente. Además, se define _samplear_ las observaciones proporcional según las etiquetas, es decir, un\n",
        "_sampleo_ estratificado para que en ambos conjunto se mantenga\n",
        "una distribución de etiquetas similares."
      ],
      "metadata": {
        "id": "LJhipQ6wqfJN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHCj6WloXafX"
      },
      "outputs": [],
      "source": [
        "# Separar los datos en un conjunto de entrenamiento y prueba.\n",
        "path_df_train, path_df_test = train_test_split(path_df, \n",
        "                                               test_size=0.2, \n",
        "                                               stratify=path_df['label'], \n",
        "                                               random_state=42)\n",
        "\n",
        "# Se reinician los índices\n",
        "path_df_train = path_df_train.reset_index(drop=True)\n",
        "path_df_test = path_df_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Número de observaciones por dataset\n",
        "# Cada dataset tiene 2 columnas, una con la ruta de imagen y otra con su etiqueta\n",
        "path_df_train.shape, path_df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKU_BmTRp4vT",
        "outputId": "61a25e53-ee7a-45db-84d7-a2f610555b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47163, 2), (11791, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificamos si ambos _dataframes_\n",
        "se encuentran balanceados respecto a los\n",
        "_labels_."
      ],
      "metadata": {
        "id": "p7MhXsgcqLcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_df_test[\"label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEXAJP6VqF0a",
        "outputId": "ad65782b-4c89-40be-8de6-36c3c59cb505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    2000\n",
              "4    2000\n",
              "0    2000\n",
              "2    2000\n",
              "5    2000\n",
              "1    1791\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_df_train[\"label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpvi5LDpqEBn",
        "outputId": "2b54cdae-9a2b-4b70-a90b-a643864abbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    8000\n",
              "5    8000\n",
              "3    8000\n",
              "0    8000\n",
              "4    8000\n",
              "1    7163\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora creamos cada _dataset_ utilizando la clase `MedicalDataset` con los parámetros respectivos, tanto\n",
        "con los inventarios de observaciones y las transformaciones, para cada uno\n",
        "de los conjuntos que buscamos crear."
      ],
      "metadata": {
        "id": "Gdp7xPa8q3Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar las transformaciones correspondientes en cada uno de los dataset.\n",
        "train_dataset = MedicalDataset(path_df_train, \"Medical-MNIST\", transform_image(\"train\"))\n",
        "test_dataset = MedicalDataset(path_df_test, \"Medical-MNIST\", transform_image(\"test\"))"
      ],
      "metadata": {
        "id": "1MfNSsR5rOH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7GBhZ2cXafX"
      },
      "outputs": [],
      "source": [
        "# Utilizar DataLoader de pytorch sobre los dataset\n",
        "# Definimos un batch size de 128 con shuffle para entrenamiento\n",
        "torch.manual_seed(666)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa1c13FgXafX"
      },
      "source": [
        "## 1.6 Creación de Funciones de Entrenamiento y Evaluación\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.researchgate.net/publication/319535615/figure/fig3/AS:536187598065664@1504848493070/A-typical-convolutional-neural-network-CNN-Architecture-for-Medical-Image-Classification.png\" width=\"500\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Ya construido todas las funciones y clases necesarias llego el momento más importante... probar la red. Para esta sección, ustedes deberán ser capaces de definir los hiperparámetros de la red, definir las funciones de perdida a utilizar, señalar el optimizador a usar y finalmente crear sus funciones para el entrenamiento y prueba. Para realizar esta parte más estructurada, seguir los siguientes puntos de forma secuencial:\n",
        "\n",
        "- [ ] Especifique los Hiperparámetros de las 3 redes. Para esta parte sea claro de su elección y señale el porqué de sus elecciones (o sea justifique el setting de sus hiperparámetros).\n",
        "- [ ] Defina los modelos a utilizar, el optimizador que utilizará para el modelo y señale la función de perdida que utilizará.\n",
        "- [X] Explique de forma breve la función que cumplen los pasos `Backward` y `Descenso del gradiente` en una red neuronal.\n",
        "- [ ] Cree una función llamado `train` que entrene a los clasificadores. Para esto, recuerde que estos modelos suelen utilizar un número de épocas, por lo que deberá generar un proceso iterativo de entrenamiento. Es importante que su función imprima las `loss` obtenidas por el modelo en cada época (si gusta puede almacenar estas losses en una lista para luego graficarlas y comparar).\n",
        "- [ ] Diseñe una función para evaluar el desempeño de las redes. Para evaluar las redes utilice solamente la métrica accuracy (para esto se le recomienda comparar la predicción con el ground truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlNtzmlJXafX"
      },
      "outputs": [],
      "source": [
        "# Especificar hyperparámetros de las redes\n",
        "in_channels_fc = 64 * 64 * 3\n",
        "in_channels_cnn = 3 \n",
        "mid_channels_cnn = [10, 50, 75, 50]\n",
        "num_classes = path_df['label'].nunique()\n",
        "lr = 3e-4\n",
        "batch_size = 128\n",
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Señale el por qué de sus elecciones (o sea justifique el setting de sus hiperparámetros)**\n",
        "\n",
        "* `in_channels_fc`: Corresponde a la dimensión de una entrada (imagen), al dejar sus pixeles dispuestos en forma lineal. Por tanto, corresponde a multiplicar la cantidad de pixeles de ancho (64) por la cantidad de pixeles de alto (64) por la cantidad de canales (3 en este caso, ya que se transforma las imágenes a RGB). Nos aseguramos que siempre la entrada será de este tamaño con las transformaciones.\n",
        "* `in_channels_cvv`: Corresponde al tamaño de la dimensión de\n",
        "canales de la imagen de entrada para las redes convolucionales anteriores. En este problema, nos encontramos\n",
        "trabajando con imágenes con 3 canales, o al menos, al leer el \n",
        "_input_ cuando creamos `MedicalDataset`, importamos las imágenes\n",
        "y luego aplicamos una transformación `RGB` en array. Por lo que\n",
        "los canales `R`, `G` y `B` fijan el tamaño del canal de\n",
        "para cada imagen en el _datasert_, el cuál será el número de canales de entrada para\n",
        "la red convolucional.\n",
        "* `num_classes`: Corresponde a la cantidad de etiquetas o clases únicas de las imágenes disponibles en el conjunto de entrenamiento, lo cual se puede obtener con `nunique` aplicado a la columna que contiene `label`.\n",
        "* `lr`: Se escoge un learning rate pequeño para asegurar convergencia alrededor de un mínimo. De todas formas, el learning rate es adaptativo debido al algoritmo de optimización escogido, por lo que pese a ser pequeño se espera una convergencia más rápida.\n",
        "* `batch_size`: No se hace uso de esta variable en este punto, pero corresponde al `batch_size` utilizado previamente para generar los `DataLoader`. Corresponde a la cantidad de datos que contiene cada lote de entrenamiento. Se escoge este valor porque permite una cantidad razonable de datos para el cálculo de gradientes.\n",
        "* `n_epochs`: Corresponde a la cantidad de épocas de entrenamiento. Esto es, la cantidad de veces que se realizará el proceso de backpropagation para cada batch de entrenamiento. Se escoge este valor debido a que un número mayor significa un mayor tiempo de entrenamiento."
      ],
      "metadata": {
        "id": "qd8rETLQt7tC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ftws9gvXafY"
      },
      "outputs": [],
      "source": [
        "# Defina los modelos a utilizar...\n",
        "torch.manual_seed(666)\n",
        "# Red 1\n",
        "model_fc = FCClassifier(in_channels_fc, [10], num_classes)\n",
        "model_cnn_1 = CNNClassifier1(in_channels_cnn, out_channels=mid_channels_cnn[0], num_classes=num_classes)\n",
        "model_cnn_2 = CNNClassifier2(in_channels_cnn, mid_channels=mid_channels_cnn, num_classes=num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "# señale la función de perdida que utilizará\n",
        "criterion_fc = nn.CrossEntropyLoss()\n",
        "criterion_cnn_1 = nn.CrossEntropyLoss()\n",
        "criterion_cnn_2 = nn.CrossEntropyLoss()\n",
        "\n",
        "# ...el optimizador que utilizará para el modelo y ...\n",
        "optimizer_fc = torch.optim.Adam(model_fc.parameters(), lr=lr)\n",
        "optimizer_cnn_1 = torch.optim.Adam(model_cnn_1.parameters(), lr=lr)\n",
        "optimizer_cnn_2 = torch.optim.Adam(model_cnn_2.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explique de forma breve la función que cumplen los pasos Backward y Descenso del gradiente en una red neuronal.**\n",
        "\n",
        "* **Descenso del Gradiente:** Algoritmo de optimización iterativo de primer orden, en cual en base a la función de pérdida, donde se deriva\n",
        "respecto a los parámetros de la red. Luego, se utiliza esta\n",
        "información para actualizar los parámetros de la red en la\n",
        "dirección contraria al gradiente, y así minimizar la función de pérdida o costo. Por lo tanto,\n",
        "en cada \"paso\" de ejecución del algoritmo, los parámetros de la\n",
        "red se actualizan según la dirección que disminuye el costo. De\n",
        "esta manera, el aprendizaje de una \"red\", es simplemente el ajuste\n",
        "de sus parámetros para disminuir la función objetivo (pérdida),\n",
        "teniendo en consideración que esta función objetivo es un proxy\n",
        "de buen _performance_ del problema para un conjunto de datos que nunca hemos visto.\n",
        "\n",
        "* **Backward:** Cualquier algoritmo de optimización de \n",
        "redes neuronales requiere computar los gradientes de la función\n",
        "de costo, o pérdida, respecto a los parámetros de la red. Debido\n",
        "a que la arquitectura de la red tiene varios parámetros, conectados\n",
        "con cadenas de operaciones (capas de la red), se debe contar\n",
        "con la capacidad de retropropagar la información del error a través\n",
        "de la red. Recordemos, el error no es más que a diferencia\n",
        "entre la predicción que se obtiene en el _forward pass_ para una\n",
        "determinada observación y su etiqueta (supervisado). Esta retropapagación del error es realizada por el\n",
        "algoritmo _backpropagation_, en el usualmente denotado _backward_\n",
        "pass, durante el entrenamiento de una red neuronal. El algoritmo\n",
        "opera construyendo un grafo directo acíclico de todos los parámetros y la relación de estos según las operaciones matemáticas\n",
        "que los conectan (i.e. suma, multiplicación, exponente, etc). Y \n",
        "así es posible saber como distribuir el error en la red para \n",
        "determinar la influencia de cada uno de los parámetros en los resultados.\n",
        "\n"
      ],
      "metadata": {
        "id": "VFL5WKi7yJpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZEuHQIMXafY"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "def train(model, train_loader, num_epochs, criterion, optimizer):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "  \n",
        "  for e in range(num_epochs):\n",
        "    train_loss = 0\n",
        "\n",
        "    #for i, (data, label) in enumerate(train_loader): #NO ENTIENDO PARA QUE HACE EL ENUMERATE\n",
        "    for data, label in train_loader:\n",
        "      data = data.to(device) \n",
        "      targets = label.to(device)\n",
        "      \n",
        "      #Forward\n",
        "      outputs = model(data)\n",
        "      loss = criterion(outputs, targets)\n",
        "      train_loss += loss\n",
        "      \n",
        "      #Backward\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      \n",
        "      # Descenso del gradiente\n",
        "      optimizer.step()\n",
        "\n",
        "    # Imprima las loss obtenidas por el modelo en cada época\n",
        "    print(f'Epoch: {e + 1}. Train Loss: {train_loss:.3f}')\n",
        "\n",
        "# Evaluate\n",
        "def evaluate(loader, model, criterion):\n",
        "\n",
        "    # Para evaluar las redes utilice solamente la métrica accuracy (para esto se le recomienda comparar la predicción con el ground truth)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for (x, y) in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.long().to(device)\n",
        "\n",
        "        y_pred = model(x)\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        top_pred = y_pred.argmax(1, keepdim=True)\n",
        "        correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "        acc += correct.float()/y.shape[0]\n",
        "\n",
        "    return float(acc.cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrpO4kYoXafY"
      },
      "source": [
        "## 1.7 Comparación de Resultados\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media2.giphy.com/media/icJA0VF7ntoEL18Jez/giphy.gif\"  width=\"200\">\n",
        "</p>\n",
        "\n",
        "Construidas las funciones de entrenamiento y evaluación, entrene a las redes que construyo y compare los resultados obtenidos con todas las redes señalando cual posee mejor rendimiento. Comente una diferencia entre las redes Fully Connected y CNN podría generar un mejor desempeño en una u otra en la tarea de clasificación de imágenes.\n",
        "\n",
        "- [X] Entrenar las redes.\n",
        "- [X] Evaluar las redes.\n",
        "- [X] Comentar los resultados obtenidos y visualizar si existe una diferencia significativa en el rendimiento debido a la naturaleza de la red."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos las tres redes que definimos arriba:"
      ],
      "metadata": {
        "id": "h936pk8YTbZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15NhglhnXafY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e4352b-01e9-4cb6-df00-06561c3be125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1. Train Loss: 335.875\n",
            "Epoch: 2. Train Loss: 200.053\n",
            "Epoch: 3. Train Loss: 107.621\n",
            "Epoch: 4. Train Loss: 60.516\n",
            "Epoch: 5. Train Loss: 51.654\n",
            "Epoch: 6. Train Loss: 46.264\n",
            "Epoch: 7. Train Loss: 40.746\n",
            "Epoch: 8. Train Loss: 37.719\n",
            "Epoch: 9. Train Loss: 35.246\n",
            "Epoch: 10. Train Loss: 32.700\n"
          ]
        }
      ],
      "source": [
        "# Red fully connected\n",
        "train(model_fc, train_loader, n_epochs, criterion_fc, optimizer_fc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Red convolucional simple de 1 capa\n",
        "train(model_cnn_1, train_loader, n_epochs, criterion_cnn_1, optimizer_cnn_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15JaAob3Sn0q",
        "outputId": "d84b379c-4d14-4e3e-ccea-5c96224764bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1. Train Loss: 48.598\n",
            "Epoch: 2. Train Loss: 8.707\n",
            "Epoch: 3. Train Loss: 5.543\n",
            "Epoch: 4. Train Loss: 4.554\n",
            "Epoch: 5. Train Loss: 3.726\n",
            "Epoch: 6. Train Loss: 3.678\n",
            "Epoch: 7. Train Loss: 2.690\n",
            "Epoch: 8. Train Loss: 2.424\n",
            "Epoch: 9. Train Loss: 2.451\n",
            "Epoch: 10. Train Loss: 2.532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Red convolucional compleja con 4 bloques convolucionales\n",
        "train(model_cnn_2, train_loader, n_epochs, criterion_cnn_2, optimizer_cnn_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZoIGmNkdtWe",
        "outputId": "69b3beff-7114-4fa8-9825-e5322398f858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1. Train Loss: 89.986\n",
            "Epoch: 2. Train Loss: 6.057\n",
            "Epoch: 3. Train Loss: 4.387\n",
            "Epoch: 4. Train Loss: 3.033\n",
            "Epoch: 5. Train Loss: 2.782\n",
            "Epoch: 6. Train Loss: 3.902\n",
            "Epoch: 7. Train Loss: 1.992\n",
            "Epoch: 8. Train Loss: 1.762\n",
            "Epoch: 9. Train Loss: 1.347\n",
            "Epoch: 10. Train Loss: 1.121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar las redes\n",
        "acc_lineal = evaluate(test_loader, model_fc, criterion_fc)\n",
        "acc_cnn_1 = evaluate(test_loader, model_cnn_1, criterion_cnn_1)\n",
        "acc_cnn_2 = evaluate(test_loader, model_cnn_2, criterion_cnn_2)"
      ],
      "metadata": {
        "id": "BI9kUo7K2Pgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy red lineal: {round(acc_lineal, 4)}\")\n",
        "print(f\"Accuracy primera red convolucional: {round(acc_cnn_1, 4)}\")\n",
        "print(f\"Accuracy segunda red convolucional: {round(acc_cnn_2, 4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bksh_TrNyvTh",
        "outputId": "7202e301-b971-4154-b023-7dba109a3fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy red lineal: 0.9791\n",
            "Accuracy primera red convolucional: 0.9984\n",
            "Accuracy segunda red convolucional: 0.9989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esFjRe0xXafY"
      },
      "source": [
        "**Comente los resultados**\n",
        "\n",
        "* Respecto del entrenamiento, se observa que para los 3 modelos la pérdida va disminuyendo en cada época, siendo la diferencia entre una época y otra cada vez menor, por lo que la red está aprendiendo convergiendo a un mínimo en la función de pérdida. De los 3 modelos definidos, el que logra una menor pérdida en entrenamiento(por ende, menor error) es la segunda red convolucional en la época 10. Destaca en todo caso la gran diferencia entre los valores de pérdida obtenidos mediante la red lineal y las redes convolucionales, siendo mejores las segundas. Esto tiene sentido, ya que las redes convolucionales intentan simular la forma en que la visión procesa imágenes, aplicando convoluciones, por lo que se espera que su desempeño sea mejor que el de una red lineal.\n",
        "\n",
        "* Respecto del accuracy en los datos de test, los tres modelos logran un muy buen desempeño, siendo levemente mejor en el caso de las redes convolucionales, de las cuales la que logra una leve mejora es la segunda red convolucional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EOw9BtRXafY"
      },
      "source": [
        "# Conclusión\n",
        "\n",
        "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/vKSR-ZakVMIAAAAC/pochitadancing-pochita.gif\">\n",
        "</p>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "74a3d55773e9b3c246359d50654dd4558d2f2e3d7e6e6cc3c5c43d1d4b65ec31"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}