{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVHYySrrXafH"
      },
      "source": [
        "<h1><center>Laboratorio 9: Los huesos de Hip贸crates Υ</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci贸n Cient铆fica para Ciencia de Datos</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHPut9OIXafL"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesor: Mat铆as Rojas y Mauricio Araneda\n",
        "- Auxiliar: Ignacio Meza D.\n",
        "- Ayudante: Rodrigo Guerra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDPGaSh3XafM"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser谩n revisados\n",
        "\n",
        "- Nombre de alumno 1: Crist贸bal Alc谩zar\n",
        "- Nombre de alumno 2: Gianina Salom贸\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibpy_PFlXafM"
      },
      "source": [
        "### **Link de repositorio de GitHub:** `https://github.com/alcazar90/sci-prog-lab`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZv01tEdXafN"
      },
      "source": [
        "### Indice \n",
        "\n",
        "1. [Temas a tratar](#Temas-a-tratar:)\n",
        "3. [Descripcci贸n del laboratorio](#Descripci贸n-del-laboratorio.)\n",
        "4. [Desarrollo](#Desarrollo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cayY65DnXafN"
      },
      "source": [
        "## Temas a tratar\n",
        "\n",
        "- Creaci贸n de clasificadores de imagenes a traves de redes Fully connected y CNN.\n",
        "- Uso de Dataloaders para la carga de datasets.\n",
        "- Comparaci贸n de Fully Connected y red convolucional.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- Fecha de entrega: 17/11/2022\n",
        "- **Grupos de 2 personas**\n",
        "- **Ausentes** deber谩n realizar la actividad solos. \n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser谩n respondidos por este medio.\n",
        "- Prohibidas las copias. \n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Creaci贸n de modelos de clasificaci贸n de im谩genes utilizando Pytorch.\n",
        "- Creaci贸n de dataloader y aplicar transformaciones sobre el dataset.\n",
        "- Comprender la diferencia entre una CNN y una Fully Connected.\n",
        "\n",
        "El laboratorio deber谩 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m谩ximo las funciones de `Pytorch`, la cual, est谩 enfocada para proyectos de Deep Learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTodVg3qXafO"
      },
      "source": [
        "# Importamos librerias utiles \n",
        "\n",
        "Comenzamos importando librer铆as utiles para la ejecuci贸n del laboratorio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SPKUy4UKXafP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models\n",
        "from torchvision import transforms as T\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVNCQAQNdVRH",
        "outputId": "9f29956e-d872-4d0d-e568-53752dc3a13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlk9g2mgXafQ"
      },
      "source": [
        "# Identificando los Huesos de Hip贸crates\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/d8/58/66/d85866cd1cc3979f94526551addf74b4.gif\" width=\"300\">\n",
        "</p>\n",
        "\n",
        "Tras el 茅xito que han tenido proponiendo modelos de machine learning en trabajos anteriores, el famoso medico Hip贸crates se ha contactado con ustedes para solicitarles ayuda para automatizar la identificaci贸n de radiograf铆as de partes humanas. Para esto, les se帽ala que le gustar铆a utilizar algoritmos de deep learning producto que Dem贸crito le se帽alo que resultan la mejor alternativa para la predicci贸n de im谩genes.\n",
        "\n",
        "En su conversaci贸n con el medico usted le comenta que ha tenido algunas clases relacionadas a Deep Learning, por esto, est谩n motivados en abordar el problema utilizando redes Fully Connected y redes convolucionales con Pytorch. Sin embargo, al anunciarle los tipos de redes que conocen, el fil贸sofo les comenta que no hab铆a escuchado muy buenos resultados por parte de las CNN, por lo que les pide que comprueben a traves de la m茅trica de accuracy que tipo de redes es mejor para la tarea de identificaci贸n de radiograf铆as. 驴Ser谩 cierto lo que dice el fil贸sofo?, Ve谩moslo en un nuevo cap铆tulo de los Laboratorios de Programaci贸n Cient铆fica para Ciencia de Datos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-HCdzXCXafR"
      },
      "source": [
        "## 1.1 Creaci贸n de Lista de Archivos\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/BJ-9w-MUVCMAAAAM/tis100-sad.gif\" width=\"300\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N4BiamZXafR"
      },
      "source": [
        "Comience revisando de forma manual cada una de las im谩genes que posee la carpeta subida a material docente. Verifique la cantidad de tipos de radiograf铆as que se tienen y la cantidad de im谩genes que dispone cada carpeta.\n",
        "\n",
        "Revisado el contenido de las im谩genes, utilice `os.listdir` para crear un `numpy.array` o un `Dataframe` que contenga las im谩genes y un label que se帽ale al tipo de radiograf铆a a la que hace referencia la imagen. Para hacer las etiquetas codifique el tipo de im谩genes en n煤meros que vayan del 0 al total de tipos de radiograf铆as, no utilice strings para codificar las etiquetas.\n",
        "\n",
        "**Ejemplo de Estructura:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hDV1stVXafS"
      },
      "source": [
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
        "</style>\n",
        "<table class=\"tg\">\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th class=\"tg-0lax\"></th>\n",
        "    <th class=\"tg-0lax\">image_path</th>\n",
        "    <th class=\"tg-0lax\">label</th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">0</td>\n",
        "    <td class=\"tg-0lax\">image1</td>\n",
        "    <td class=\"tg-0lax\">1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">1</td>\n",
        "    <td class=\"tg-0lax\">image2</td>\n",
        "    <td class=\"tg-0lax\">0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">2</td>\n",
        "    <td class=\"tg-0lax\">image3</td>\n",
        "    <td class=\"tg-0lax\">2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">3</td>\n",
        "    <td class=\"tg-0lax\">image4</td>\n",
        "    <td class=\"tg-0lax\">0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">4</td>\n",
        "    <td class=\"tg-0lax\">image5</td>\n",
        "    <td class=\"tg-0lax\">4</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wjs3VE4XafS"
      },
      "outputs": [],
      "source": [
        "# C贸digo Aqu铆\n",
        "#for i, category in enumerate(os.listdir(\"ruta de carpeta\")):\n",
        "#    for image i n os.listdir(\"ruta de subcarpetas\"):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip gdrive/My\\ Drive/Medical-MNIST.zip"
      ],
      "metadata": {
        "id": "PMeSo2L0djiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_df = pd.DataFrame([\n",
        "    {\n",
        "        \"image_path\": \"/\".join(f.split(\"/\")[1:]),\n",
        "        \"label\": f.split(\"/\")[1]\n",
        "    }\n",
        "    for f in glob(\"Medical-MNIST/*/*.jpeg\")\n",
        "])\n",
        "\n",
        "path_df[\"label\"] = path_df[\"label\"].replace(\n",
        "    sorted(path_df[\"label\"].unique()),\n",
        "    range(path_df[\"label\"].nunique())\n",
        ")\n",
        "\n",
        "path_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gLt8vEF1bDjB",
        "outputId": "c9491f54-ef8f-429c-b79a-2de89e3e2c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         image_path  label\n",
              "0  Hand/008745.jpeg      4\n",
              "1  Hand/007006.jpeg      4\n",
              "2  Hand/004826.jpeg      4\n",
              "3  Hand/002630.jpeg      4\n",
              "4  Hand/009559.jpeg      4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51169d7c-528c-4f29-8008-0314f7427859\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hand/008745.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hand/007006.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hand/004826.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hand/002630.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hand/009559.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51169d7c-528c-4f29-8008-0314f7427859')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51169d7c-528c-4f29-8008-0314f7427859 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51169d7c-528c-4f29-8008-0314f7427859');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_df[\"label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Br7AHMEklO3",
        "outputId": "edace28b-663e-42b1-e855-bc4973fe8875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    10000\n",
              "3    10000\n",
              "2    10000\n",
              "0    10000\n",
              "5    10000\n",
              "1     8954\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Las clases se encuentran balanceadas"
      ],
      "metadata": {
        "id": "iDO8Hx9Ukozl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrftPjphXafT"
      },
      "source": [
        "## 1.2 Creaci贸n de Dataset\n",
        "\n",
        "Tomando en cuenta la estructura de datos desarrollada en el punto 1.1, construya la clase `MedicalDataset()` que cumpla los siguientes puntos:\n",
        "\n",
        "- [X] Poseer un `__init__` en el que se almacene `estructura` creada en 1.1, la `raiz` de la carpeta y una funci贸n que permita transformar el dataset (de esto no se preocupe mucho, ya que solamente debe almacenar una funci贸n en el atributo).\n",
        "- [X] La clase debe ser capaz de entregar la cantidad de elementos a traves de `__len__`.\n",
        "- [X] Debe poseer el m茅todo `__getitem__` que retorne una tupla con la imagen y su correspondiente etiqueta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjYcR-V4XafT"
      },
      "outputs": [],
      "source": [
        "# C贸digo Aqu铆\n",
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, estructura, raiz, transform=False):\n",
        "        self.estructura = estructura\n",
        "        self.raiz = raiz\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # Un poco de ayuda para cargar la imagen\n",
        "        img_path = os.path.join(self.raiz, self.estructura[\"image_path\"][idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.estructura[\"label\"][idx]\n",
        "        \n",
        "        # Auida para realizar la transformaci贸n\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.estructura.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "henHo3G7XafT"
      },
      "source": [
        "## 1.3 Prueba del MedicalDataset\n",
        "\n",
        "Con la clase construida en el punto 1.2, verifique su funcionamiento cargando el dataset y realizando las transformaciones que entrega la funci贸n `transform_image`. Compruebe a trav茅s de un ejemplo las transformaciones aplicadas en la imagen, comentando la funci贸n que cumple `MedicalDataset` y si es posible observar todas las transformaciones aplicadas con la funci贸n `transform_image`.\n",
        "\n",
        "- [X] Probar la clase MedicalDataset y aplicando una transformaci贸n de \"train\"\n",
        "- [X] Plotear un ejemplo del MedicalDataset.\n",
        "\n",
        "**Funci贸n para transformar las imagenes:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eAV6styXafU"
      },
      "outputs": [],
      "source": [
        "def transform_image(stage = None):\n",
        "    \n",
        "    if stage == \"train\":\n",
        "        Tr_img = T.Compose([\n",
        "                  T.Resize(size = (64,64)),\n",
        "                  #T.Resize(size = (256,256)),\n",
        "                  T.RandomRotation(degrees = (-20,+20)),\n",
        "                  T.ToTensor()])\n",
        "        \n",
        "    elif stage == \"test\" or stage == \"val\":\n",
        "        Tr_img = T.Compose([\n",
        "                  T.Resize(size = (64,64)), \n",
        "                  T.ToTensor()]) \n",
        "\n",
        "    return Tr_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBVMvOlXafU"
      },
      "source": [
        "**C贸digo para obtener un ejemplo:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0bNfbp5XafU"
      },
      "outputs": [],
      "source": [
        "# Prueba del dataset\n",
        "dataset = MedicalDataset(path_df, \"Medical-MNIST\", transform_image(\"train\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_image, example_label = dataset[0]\n",
        "# Utilice plotly para plotear un ejemplo\n",
        "px.imshow(example_image.numpy().T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "SLwYV8LaReb6",
        "outputId": "0b063b78-6b7d-41fe-d32f-2b993333656c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"373acb1f-d8a8-41d9-8171-61153328656c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"373acb1f-d8a8-41d9-8171-61153328656c\")) {                    Plotly.newPlot(                        \"373acb1f-d8a8-41d9-8171-61153328656c\",                        [{\"name\":\"0\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<extra></extra>\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('373acb1f-d8a8-41d9-8171-61153328656c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imagen sin transformaci贸n\n",
        "dataset_sint = MedicalDataset(path_df, \"Medical-MNIST\")\n",
        "example_image_sint, example_label_sint = dataset_sint[0]\n",
        "px.imshow(example_image_sint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "a8f9BPtiSwQG",
        "outputId": "7efe0228-408f-44d4-83fd-8b17d5be2edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"9099af38-e9f0-4482-82f0-11369b71d447\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9099af38-e9f0-4482-82f0-11369b71d447\")) {                    Plotly.newPlot(                        \"9099af38-e9f0-4482-82f0-11369b71d447\",                        [{\"name\":\"0\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: [%{z[0]}, %{z[1]}, %{z[2]}]<extra></extra>\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9099af38-e9f0-4482-82f0-11369b71d447');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCmn_aWLXafW"
      },
      "source": [
        "> Comente que realiza la clase construida y las transformaciones aplicadas.\n",
        "\n",
        "La clase `MedicalDataset` permite construir el _dataset_, en base a im谩genes que se encuentran organizadas en una estructura de carpetas (_aka ImageFolder_), y cuyas etiquetas son los nombres\n",
        "de los directorios. El funcionamiento de la clase es navegar\n",
        "por esta estructura y cargar la informaci贸n de las\n",
        "im谩genes de manera organizada y con sus etiquetas respectivas:\n",
        "\n",
        "* Una observaci贸n en el _dataset_ se constituye por una imagen\n",
        "representada como un `array`, o un `Tensor`, y una etiqueta representada por un entero. Por lo que una observaci贸n se\n",
        "conforma por la siguiente tupla: `(imagen, etiqueta)`.\n",
        "* La clase requiere un _dataframe_ (`path_df`) con dos columnas, donde la\n",
        "primera tiene el nombre de la imagen y la segunda su etiqueta. Es\n",
        "decir, una tabla con el inventario de todas las observaciones del _dataset_.\n",
        "* En base la informaci贸n anterior, espec铆ficamente la primera \n",
        "columna, la clase `MedicalDataset` tiene la capacidad de\n",
        "reconstruir el `path` donde se encuentra la imagen. Esto en conjunto con el atributo `self.raiz`, que indica la ubicaci贸n del directorio donde se encuentra la estructura de carpetas con las im谩genes, y as铆 luego importar la imagen en un arreglo `numpy` usando `Image.open(img_path).convert('RGB')`.\n",
        "* La funcionalidad anterior es realizada por el m茅todo `MedicalDataset.__getitem__()`. Lo que permite acceder por 铆ndices\n",
        "a las observaciones del _dataset_. Por ejemplo,  `image_0, label_0 = MedicalDataset(..)[0]` nos entrega la tupla correspondiente\n",
        "a la primera observaci贸n.\n",
        "* Un punto a destacar de lo anterior, la clase `MedicalDataset` tiene la capacidad de cargar las observaciones que uno requiere, pero en ning煤n caso, como se puede apreciar en el c贸digo, importa todas las im谩genes cuando se inicializa. Esto no es un error, por el tama帽o de las im谩genes, es m谩s ventajoso que esta clase funcione\n",
        "como un rutero con capacidad de catrastrar la informaci贸n y cargarla cuando se requiere para no colapsar la memor铆a. \n",
        "* Luego veremos que hay una especie de orquestado que aprovecha\n",
        "la funcionalidad de `MedicalDataset` para ir solicitando\n",
        "observaciones a medida que las necesita: `DataLoader`.\n",
        "* Es posible saber la cantidad de observaciones del _dataset_ utilizando el m茅todo `MedicalDataset.__len__(self)`, que inspecciona la cantidad de filas del\n",
        "_dataframe_ inventario de todas las observaciones.\n",
        "* Adem谩s, es posible posterior a importar una imagen, aplicar\n",
        "un conjunto de transformaciones como rotaciones, resize, entre \n",
        "otras. Esto se realiza con el atributo `self.transform`.\n",
        "\n",
        "**Respecto a las transformaciones:**\n",
        "\n",
        "* Las transformaciones aplicadas consisten en realizar una rotaci贸n de 20 grados de la imagen original, y aplicar un redimensionamiento a 64x64 pixeles.Adem谩s,  se cambia la estructura del tipo de dato de la imagen almacenada originalmente en numpy, a tipo de dato Tensor.\n",
        "\n",
        "* En este caso particular no cambia el tama帽o de la imagen, ya que todas las im谩genes son originalmente de 64x64, pero se debe tener en cuenta esta transformaci贸n en caso de que la red se entrene con otro conjunto de datos, o que se eval煤e con im谩genes de otro tama帽o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WARo1zQBXafW"
      },
      "source": [
        "## 1.4 Creaci贸n de Clasificadores\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/01/conv-full-layer.gif\" width=\"300\">\n",
        "</p>\n",
        "\n",
        "A continuaci贸n, deben construir tres clasificadores con los que deber谩n verificar cu谩l de las arquitecturas posee un mejor desempe帽o para la tarea de clasificaci贸n de im谩genes. Para la construcci贸n considere los siguientes puntos:\n",
        "\n",
        "- [X] Se帽ale cual es el objetivo del `forward` en este tipo de redes, sea breve para su explicaci贸n.\n",
        "- [X] Construir una red Fully Connected para solucionar el problema de clasificaci贸n. Para esta parte se le aconseja que rellene el esqueleto dispuesto m谩s abajo y que lleva el nombre de `FCClassifier`, en el deber谩 rellenar con la dimensi贸n de las capas ocultas y verificar cual ser谩 el tama帽o de la entrada.\n",
        "- [X] Construya una red convolucional **simple** (no m谩s de una capa convolucional) para la tarea de clasificaci贸n de im谩genes, para esto basen su c贸digo en la clase del d铆a `09-11-2022`.\n",
        "- [X] Crear una red convolucional m谩s compleja. Para esta parte tienen completa libertad en la construcci贸n de su red, lo 煤nico que debe cumplir es que sea convolucional.\n",
        "\n",
        "**Esqueletos Propuestos:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo del m茅todo `forward`**: El\n",
        "_forward_ pass son todas las operaciones\n",
        "que se aplican al _input_ para llegar al output, o predicci贸n. Corresponde a c贸mo la informaci贸n del _input_ fluye a trav茅s de las capas que conforman la arquitectura de la red, aplicandose las operaciones lineales usando los par谩metros (iniciados generalmente de forma aleatoria cuando se crea la red), funciones de activaci贸n no lineales, y otros mecanismos que se utilizan cuando se computan las predicciones durante entrenamiento (i.e. capas de dropout). Su retorno incluye de forma general la salida de la red para una entrada dada, que puede haber pasado o no por una funci贸n de activaci贸n de la capa de salida, dependiendo de la implementaci贸n."
      ],
      "metadata": {
        "id": "EurknL02mIYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbl06W5jXafW"
      },
      "outputs": [],
      "source": [
        "# Construir una red Fully Connected para solucionar el problema de clasificaci贸n.\n",
        "# Para esta parte se le aconseja que rellene el esqueleto dispuesto m谩s abajo\n",
        "# y que lleva el nombre de FCClassifier...\n",
        "class FCClassifier(nn.Module):\n",
        "    # ... en 茅l deber谩 rellenar con la dimensi贸n de las capas ocultas...:\n",
        "    # R: Para ello agregamos un nuevo par谩metro de lista que contiene\n",
        "    # la cantidad de neuronas de cada capa oculta.\n",
        "    def __init__(self, in_channels: int, hidden_sizes: list, num_classes: int):\n",
        "        super(FCClassifier, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # ...y verificar cual ser谩 el tama帽o de la entrada.\n",
        "        assert isinstance(in_channels, int) and in_channels > 0\n",
        "\n",
        "        # Se asigna entrada a primera capa oculta\n",
        "        self.lin1 =  nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_sizes[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Se genera las siguientes capas ocultas\n",
        "        if len(hidden_sizes) > 1:\n",
        "          self.other_lin = torch.nn.ModuleList([\n",
        "              nn.Sequential(\n",
        "                  nn.Linear(hidden_sizes[i], s),\n",
        "                  nn.ReLU()\n",
        "              )\n",
        "              for i, s in enumerate(hidden_sizes[1:])         \n",
        "          ])\n",
        "\n",
        "        else:\n",
        "          self.other_lin = torch.nn.ModuleList([])\n",
        "\n",
        "        # Se cambia nombre a capa de salida\n",
        "        self.lin_out = nn.Linear(hidden_sizes[-1], num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.lin1(self.flatten(x))\n",
        "\n",
        "        # Otras capas ocultas\n",
        "        for lin in self.other_lin:\n",
        "          out = lin(x)\n",
        "\n",
        "        # Capa de salida (no se aplica activaci贸n)\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imvkOHgwXafW"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier1(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, num_classes):\n",
        "      super().__init__()\n",
        "      self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                                  padding=1)\n",
        "      # kernel_size=3 + padding=1 -> conserva las dimensiones de la imagen\n",
        "      # original luego de aplicar la capa convolucional\n",
        "      self.fc = nn.Linear(64 * 64 * out_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = F.relu(self.conv_layer(x))\n",
        "      out = torch.flatten(out, 1)\n",
        "      out = self.fc(out)\n",
        "      return out\n",
        "\n",
        "    \n",
        "class CNNClassifier2(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, mid_channels, num_classes):\n",
        "      super().__init__()\n",
        "      # Combinaci贸n kernel_size=3 y padding=1 permiten conservar\n",
        "      # las dimensiones del input una vez aplicada la capa convolucional\n",
        "      # Elecci贸n: facilita pasar luego a la capa lineal debido a que no\n",
        "      # hay que llevar un \"conteo\" de la p茅rdida de pixeles cada vez que\n",
        "      # se aplica un kernel en una capa convolucional. Raz贸n pr谩ctica.\n",
        "      self.conv_layer1 = nn.Conv2d(in_channels, mid_channels[0], kernel_size=3,\n",
        "                                   padding=1)\n",
        "\n",
        "      # Se genera los siguientes bloques convolcuionales intermedios \n",
        "      if len(mid_channels) > 1:\n",
        "        self.other_conv = torch.nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.MaxPool2d(2, 2),\n",
        "                nn.Conv2d(mid_channels[i], mid_channels[i+1], kernel_size=3,\n",
        "                          padding=1),\n",
        "                nn.ReLU(),\n",
        "                # La introducci贸n de m谩s bloques puede provocar overfitting\n",
        "                # para alivianar este problema se agrega una capa de dropout\n",
        "                # como regularizaci贸n (50% de las neuronas se apaga -> p=0.5)\n",
        "                nn.Dropout(p=0.5)\n",
        "          )\n",
        "            for i, s in enumerate(mid_channels[:-1])         \n",
        "        ])\n",
        "        # Factor que lleva la cuenta de cuantas veces las dimensiones\n",
        "        # de la imagen inicia HxW son reducidas a la mitad por la capa de\n",
        "        # nn.MaxPool2d(2,2)\n",
        "        self.shrink_factor = 2 ** (len(mid_channels)-1)\n",
        "\n",
        "      else:\n",
        "        self.other_conv = torch.nn.ModuleList([])\n",
        "        # Si no hay capas convolucionales, el factor es 1 (no se reduce)\n",
        "        self.shrink_factor = 2 ** 0\n",
        "\n",
        "      self.fc = nn.Linear(int(64 / self.shrink_factor) * int(64 / self.shrink_factor) * mid_channels[-1], num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = F.relu(self.conv_layer1(x))\n",
        "\n",
        "      # Aplicar bloques convolucionales intermedios: conv - act - pool\n",
        "      for conv in self.other_conv:\n",
        "        out = conv(out)\n",
        "\n",
        "      # Aplanar el vector\n",
        "      out = torch.flatten(out, 1)\n",
        "\n",
        "      # Apicar capa fully connected\n",
        "      out = self.fc(out)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos un _sanity check_ con las arquitecturas convolucionales \n",
        "para verificar si el dise帽o de las capas est谩n correctamente\n",
        "especificados seg煤n las dimensiones del _input_.\n",
        "\n",
        "Para esto, utilizamos una solo imagen a color de ejemplo (`3x64x64`), y agregamos una dimensi贸n adicional que indica\n",
        "el tama帽o del _batch_ (`1x3x64x64`). Esto lo podemos\n",
        "hacer facilmente usando el m茅todo `.unsqueeze(0)` para agregar\n",
        "una dimensi贸n a un tensor en la posici贸n 0 (aka primera). Luego, probamos\n",
        "si la informaci贸n fluye corrrectamente en el _forward pass_:"
      ],
      "metadata": {
        "id": "HGouKPPhQOnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Si no arroja error significa que la arquitectura responde a las\n",
        "# dimensiones del dataset\n",
        "torch.manual_seed(666)\n",
        "\n",
        "cnn = CNNClassifier1(in_channels=3, out_channels=10, num_classes=4)\n",
        "cnn(dataset[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHPwYVY3Oe6H",
        "outputId": "af9261e7-023d-4601-8b01-1f95921b1ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0212,  0.0151, -0.1896, -0.0924]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, probamos la red convolucional m谩s compleja que\n",
        "permite agregar bloques convoluciones intermedio, cada bloque\n",
        "esta compuesto por:\n",
        "\n",
        "> `nn.MaxPool2d(2,2)` -> `nn.Conv2d(mid_channels[i], mid_channels[i+1])` -> `nn.ReLU() `\n",
        "\n",
        "Notemos que siempre reducimos el tama帽o de la imagen sobre\n",
        "la que se esta aplicando el kernel a la mitad (i.e. 64 - 32 - 16),\n",
        "tantos bloques apliquemos (i.e. `len(mid_channels`). Los par谩metros de cada capa convolucional se encuentran especificados\n",
        "en los elementos de `mid_channels`.\n",
        "\n",
        "Ahora probemos el funcionamiento con un `len(mid_channels)=1` y\n",
        "cuyo tama帽o de canal es `10`. Esto debiera ser equivalente a la\n",
        "primera arquitectura convolucional con `out_channels=10`. Donde\n",
        "t茅cnicamente no utilizamos ning煤n bloque convolucional."
      ],
      "metadata": {
        "id": "rwu_iMwrWvcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(666)\n",
        "\n",
        "cnn2 = CNNClassifier2(in_channels=3, mid_channels=[10], num_classes=4)\n",
        "cnn2(dataset[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMvM2fyLVz7u",
        "outputId": "cc5539ea-df8b-48c5-b4bb-f9943fce526b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0212,  0.0151, -0.1896, -0.0924]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los resultados son iguales.\n",
        "\n",
        "Ahora probemos una red m谩s compleja, con 4 bloques \n",
        "convolucionales: `mid_channels=[10, 50, 75, 50]`."
      ],
      "metadata": {
        "id": "HFURBH0_ZofU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(666)\n",
        "\n",
        "cnn2 = CNNClassifier2(in_channels=3, mid_channels=[10, 50, 75, 50], num_classes=4)\n",
        "cnn2(dataset[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDNneLcjZwfi",
        "outputId": "b6d81169-74be-47b8-94c8-b6a544064ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0573,  0.0896,  0.0545, -0.0064]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ambas implementaciones de redes convolucionales est谩n\n",
        "funcionando correctamente!\n"
      ],
      "metadata": {
        "id": "s5NHBJM8fCB7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxnVPPB6XafX"
      },
      "source": [
        "## 1.5 Separando Datos para el Entrenamiento\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://c.tenor.com/Esn7Jif-ZLQAAAAC/separate-square.gif\" width=\"200\">\n",
        "</p>\n",
        "\n",
        "Utilizando un Holdout a su gusto, separe los datos en un conjunto de entrenamiento y de testing. Aplique las transformaciones correspondientes usando `transform_image` para cada conjunto de datos y utilice `torch.utils.data.DataLoader` para crear un objeto iterable del dataset.\n",
        "\n",
        "- [X] Definir el Holdout a utilizar.\n",
        "- [X] Separar los datos en un conjunto de entrenamiento y prueba.\n",
        "- [X] Aplicar las transformaciones correspondientes en cada uno de los dataset.\n",
        "- [X] Utilizar `DataLoader` de pytorch sobre los dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definir holdout a utilizar**\n",
        "\n",
        "Se crean _dataframes_ con inventarios de observaciones para el conjunto de entrenamiento y prueba siguiendo una proporci贸n de 80-20 respectivamente. Adem谩s, se define _samplear_ las observaciones proporcional seg煤n las etiquetas, es decir, un\n",
        "_sampleo_ estratificado para que en ambos conjunto se mantenga\n",
        "una distribuci贸n de etiquetas similares."
      ],
      "metadata": {
        "id": "LJhipQ6wqfJN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHCj6WloXafX"
      },
      "outputs": [],
      "source": [
        "# Separar los datos en un conjunto de entrenamiento y prueba.\n",
        "path_df_train, path_df_test = train_test_split(path_df, \n",
        "                                               test_size=0.2, \n",
        "                                               stratify=path_df['label'], \n",
        "                                               random_state=42)\n",
        "\n",
        "# Se reinician los 铆ndices\n",
        "path_df_train = path_df_train.reset_index(drop=True)\n",
        "path_df_test = path_df_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# N煤mero de observaciones por dataset\n",
        "# Cada dataset tiene 2 columnas, una con la ruta de imagen y otra con su etiqueta\n",
        "path_df_train.shape, path_df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKU_BmTRp4vT",
        "outputId": "61a25e53-ee7a-45db-84d7-a2f610555b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47163, 2), (11791, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificamos si ambos _dataframes_\n",
        "se encuentran balanceados respecto a los\n",
        "_labels_."
      ],
      "metadata": {
        "id": "p7MhXsgcqLcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_df_test[\"label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEXAJP6VqF0a",
        "outputId": "ad65782b-4c89-40be-8de6-36c3c59cb505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    2000\n",
              "4    2000\n",
              "0    2000\n",
              "2    2000\n",
              "5    2000\n",
              "1    1791\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_df_train[\"label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpvi5LDpqEBn",
        "outputId": "2b54cdae-9a2b-4b70-a90b-a643864abbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    8000\n",
              "5    8000\n",
              "3    8000\n",
              "0    8000\n",
              "4    8000\n",
              "1    7163\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora creamos cada _dataset_ utilizando la clase `MedicalDataset` con los par谩metros respectivos, tanto\n",
        "con los inventarios de observaciones y las transformaciones, para cada uno\n",
        "de los conjuntos que buscamos crear."
      ],
      "metadata": {
        "id": "Gdp7xPa8q3Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar las transformaciones correspondientes en cada uno de los dataset.\n",
        "train_dataset = MedicalDataset(path_df_train, \"Medical-MNIST\", transform_image(\"train\"))\n",
        "test_dataset = MedicalDataset(path_df_test, \"Medical-MNIST\", transform_image(\"test\"))"
      ],
      "metadata": {
        "id": "1MfNSsR5rOH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7GBhZ2cXafX"
      },
      "outputs": [],
      "source": [
        "# Utilizar DataLoader de pytorch sobre los dataset\n",
        "# Definimos un batch size de 128 con shuffle para entrenamiento\n",
        "torch.manual_seed(666)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa1c13FgXafX"
      },
      "source": [
        "## 1.6 Creaci贸n de Funciones de Entrenamiento y Evaluaci贸n\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.researchgate.net/publication/319535615/figure/fig3/AS:536187598065664@1504848493070/A-typical-convolutional-neural-network-CNN-Architecture-for-Medical-Image-Classification.png\" width=\"500\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Ya construido todas las funciones y clases necesarias llego el momento m谩s importante... probar la red. Para esta secci贸n, ustedes deber谩n ser capaces de definir los hiperpar谩metros de la red, definir las funciones de perdida a utilizar, se帽alar el optimizador a usar y finalmente crear sus funciones para el entrenamiento y prueba. Para realizar esta parte m谩s estructurada, seguir los siguientes puntos de forma secuencial:\n",
        "\n",
        "- [ ] Especifique los Hiperpar谩metros de las 3 redes. Para esta parte sea claro de su elecci贸n y se帽ale el porqu茅 de sus elecciones (o sea justifique el setting de sus hiperpar谩metros).\n",
        "- [ ] Defina los modelos a utilizar, el optimizador que utilizar谩 para el modelo y se帽ale la funci贸n de perdida que utilizar谩.\n",
        "- [X] Explique de forma breve la funci贸n que cumplen los pasos `Backward` y `Descenso del gradiente` en una red neuronal.\n",
        "- [ ] Cree una funci贸n llamado `train` que entrene a los clasificadores. Para esto, recuerde que estos modelos suelen utilizar un n煤mero de 茅pocas, por lo que deber谩 generar un proceso iterativo de entrenamiento. Es importante que su funci贸n imprima las `loss` obtenidas por el modelo en cada 茅poca (si gusta puede almacenar estas losses en una lista para luego graficarlas y comparar).\n",
        "- [ ] Dise帽e una funci贸n para evaluar el desempe帽o de las redes. Para evaluar las redes utilice solamente la m茅trica accuracy (para esto se le recomienda comparar la predicci贸n con el ground truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlNtzmlJXafX"
      },
      "outputs": [],
      "source": [
        "# Especificar hyperpar谩metros de las redes\n",
        "in_channels_fc = 64 * 64 * 3\n",
        "in_channels_cnn = 3 \n",
        "mid_channels_cnn = [10, 50, 75, 50]\n",
        "num_classes = path_df['label'].nunique()\n",
        "lr = 3e-4\n",
        "batch_size = 128\n",
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Se帽ale el por qu茅 de sus elecciones (o sea justifique el setting de sus hiperpar谩metros)**\n",
        "\n",
        "* `in_channels_fc`: Corresponde a la dimensi贸n de una entrada (imagen), al dejar sus pixeles dispuestos en forma lineal. Por tanto, corresponde a multiplicar la cantidad de pixeles de ancho (64) por la cantidad de pixeles de alto (64) por la cantidad de canales (3 en este caso, ya que se transforma las im谩genes a RGB). Nos aseguramos que siempre la entrada ser谩 de este tama帽o con las transformaciones.\n",
        "* `in_channels_cvv`: Corresponde al tama帽o de la dimensi贸n de\n",
        "canales de la imagen de entrada para las redes convolucionales anteriores. En este problema, nos encontramos\n",
        "trabajando con im谩genes con 3 canales, o al menos, al leer el \n",
        "_input_ cuando creamos `MedicalDataset`, importamos las im谩genes\n",
        "y luego aplicamos una transformaci贸n `RGB` en array. Por lo que\n",
        "los canales `R`, `G` y `B` fijan el tama帽o del canal de\n",
        "para cada imagen en el _datasert_, el cu谩l ser谩 el n煤mero de canales de entrada para\n",
        "la red convolucional.\n",
        "* `num_classes`: Corresponde a la cantidad de etiquetas o clases 煤nicas de las im谩genes disponibles en el conjunto de entrenamiento, lo cual se puede obtener con `nunique` aplicado a la columna que contiene `label`.\n",
        "* `lr`: Se escoge un learning rate peque帽o para asegurar convergencia alrededor de un m铆nimo. De todas formas, el learning rate es adaptativo debido al algoritmo de optimizaci贸n escogido, por lo que pese a ser peque帽o se espera una convergencia m谩s r谩pida.\n",
        "* `batch_size`: No se hace uso de esta variable en este punto, pero corresponde al `batch_size` utilizado previamente para generar los `DataLoader`. Corresponde a la cantidad de datos que contiene cada lote de entrenamiento. Se escoge este valor porque permite una cantidad razonable de datos para el c谩lculo de gradientes.\n",
        "* `n_epochs`: Corresponde a la cantidad de 茅pocas de entrenamiento. Esto es, la cantidad de veces que se realizar谩 el proceso de backpropagation para cada batch de entrenamiento. Se escoge este valor debido a que un n煤mero mayor significa un mayor tiempo de entrenamiento."
      ],
      "metadata": {
        "id": "qd8rETLQt7tC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ftws9gvXafY"
      },
      "outputs": [],
      "source": [
        "# Defina los modelos a utilizar...\n",
        "torch.manual_seed(666)\n",
        "# Red 1\n",
        "model_fc = FCClassifier(in_channels_fc, [10], num_classes)\n",
        "model_cnn_1 = CNNClassifier1(in_channels_cnn, out_channels=mid_channels_cnn[0], num_classes=num_classes)\n",
        "model_cnn_2 = CNNClassifier2(in_channels_cnn, mid_channels=mid_channels_cnn, num_classes=num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "# se帽ale la funci贸n de perdida que utilizar谩\n",
        "criterion_fc = nn.CrossEntropyLoss()\n",
        "criterion_cnn_1 = nn.CrossEntropyLoss()\n",
        "criterion_cnn_2 = nn.CrossEntropyLoss()\n",
        "\n",
        "# ...el optimizador que utilizar谩 para el modelo y ...\n",
        "optimizer_fc = torch.optim.Adam(model_fc.parameters(), lr=lr)\n",
        "optimizer_cnn_1 = torch.optim.Adam(model_cnn_1.parameters(), lr=lr)\n",
        "optimizer_cnn_2 = torch.optim.Adam(model_cnn_2.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explique de forma breve la funci贸n que cumplen los pasos Backward y Descenso del gradiente en una red neuronal.**\n",
        "\n",
        "* **Descenso del Gradiente:** Algoritmo de optimizaci贸n iterativo de primer orden, en cual en base a la funci贸n de p茅rdida, donde se deriva\n",
        "respecto a los par谩metros de la red. Luego, se utiliza esta\n",
        "informaci贸n para actualizar los par谩metros de la red en la\n",
        "direcci贸n contraria al gradiente, y as铆 minimizar la funci贸n de p茅rdida o costo. Por lo tanto,\n",
        "en cada \"paso\" de ejecuci贸n del algoritmo, los par谩metros de la\n",
        "red se actualizan seg煤n la direcci贸n que disminuye el costo. De\n",
        "esta manera, el aprendizaje de una \"red\", es simplemente el ajuste\n",
        "de sus par谩metros para disminuir la funci贸n objetivo (p茅rdida),\n",
        "teniendo en consideraci贸n que esta funci贸n objetivo es un proxy\n",
        "de buen _performance_ del problema para un conjunto de datos que nunca hemos visto.\n",
        "\n",
        "* **Backward:** Cualquier algoritmo de optimizaci贸n de \n",
        "redes neuronales requiere computar los gradientes de la funci贸n\n",
        "de costo, o p茅rdida, respecto a los par谩metros de la red. Debido\n",
        "a que la arquitectura de la red tiene varios par谩metros, conectados\n",
        "con cadenas de operaciones (capas de la red), se debe contar\n",
        "con la capacidad de retropropagar la informaci贸n del error a trav茅s\n",
        "de la red. Recordemos, el error no es m谩s que a diferencia\n",
        "entre la predicci贸n que se obtiene en el _forward pass_ para una\n",
        "determinada observaci贸n y su etiqueta (supervisado). Esta retropapagaci贸n del error es realizada por el\n",
        "algoritmo _backpropagation_, en el usualmente denotado _backward_\n",
        "pass, durante el entrenamiento de una red neuronal. El algoritmo\n",
        "opera construyendo un grafo directo ac铆clico de todos los par谩metros y la relaci贸n de estos seg煤n las operaciones matem谩ticas\n",
        "que los conectan (i.e. suma, multiplicaci贸n, exponente, etc). Y \n",
        "as铆 es posible saber como distribuir el error en la red para \n",
        "determinar la influencia de cada uno de los par谩metros en los resultados.\n",
        "\n"
      ],
      "metadata": {
        "id": "VFL5WKi7yJpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZEuHQIMXafY"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "def train(model, train_loader, num_epochs, criterion, optimizer):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "  \n",
        "  for e in range(num_epochs):\n",
        "    train_loss = 0\n",
        "\n",
        "    #for i, (data, label) in enumerate(train_loader): #NO ENTIENDO PARA QUE HACE EL ENUMERATE\n",
        "    for data, label in train_loader:\n",
        "      data = data.to(device) \n",
        "      targets = label.to(device)\n",
        "      \n",
        "      #Forward\n",
        "      outputs = model(data)\n",
        "      loss = criterion(outputs, targets)\n",
        "      train_loss += loss\n",
        "      \n",
        "      #Backward\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      \n",
        "      # Descenso del gradiente\n",
        "      optimizer.step()\n",
        "\n",
        "    # Imprima las loss obtenidas por el modelo en cada 茅poca\n",
        "    print(f'Epoch: {e + 1}. Train Loss: {train_loss:.3f}')\n",
        "\n",
        "# Evaluate\n",
        "def evaluate(loader, model, criterion):\n",
        "\n",
        "    # Para evaluar las redes utilice solamente la m茅trica accuracy (para esto se le recomienda comparar la predicci贸n con el ground truth)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for (x, y) in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.long().to(device)\n",
        "\n",
        "        y_pred = model(x)\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        top_pred = y_pred.argmax(1, keepdim=True)\n",
        "        correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "        acc += correct.float()/y.shape[0]\n",
        "\n",
        "    return float(acc.cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrpO4kYoXafY"
      },
      "source": [
        "## 1.7 Comparaci贸n de Resultados\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media2.giphy.com/media/icJA0VF7ntoEL18Jez/giphy.gif\"  width=\"200\">\n",
        "</p>\n",
        "\n",
        "Construidas las funciones de entrenamiento y evaluaci贸n, entrene a las redes que construyo y compare los resultados obtenidos con todas las redes se帽alando cual posee mejor rendimiento. Comente una diferencia entre las redes Fully Connected y CNN podr铆a generar un mejor desempe帽o en una u otra en la tarea de clasificaci贸n de im谩genes.\n",
        "\n",
        "- [X] Entrenar las redes.\n",
        "- [X] Evaluar las redes.\n",
        "- [X] Comentar los resultados obtenidos y visualizar si existe una diferencia significativa en el rendimiento debido a la naturaleza de la red."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos las tres redes que definimos arriba:"
      ],
      "metadata": {
        "id": "h936pk8YTbZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15NhglhnXafY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e4352b-01e9-4cb6-df00-06561c3be125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1. Train Loss: 335.875\n",
            "Epoch: 2. Train Loss: 200.053\n",
            "Epoch: 3. Train Loss: 107.621\n",
            "Epoch: 4. Train Loss: 60.516\n",
            "Epoch: 5. Train Loss: 51.654\n",
            "Epoch: 6. Train Loss: 46.264\n",
            "Epoch: 7. Train Loss: 40.746\n",
            "Epoch: 8. Train Loss: 37.719\n",
            "Epoch: 9. Train Loss: 35.246\n",
            "Epoch: 10. Train Loss: 32.700\n"
          ]
        }
      ],
      "source": [
        "# Red fully connected\n",
        "train(model_fc, train_loader, n_epochs, criterion_fc, optimizer_fc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Red convolucional simple de 1 capa\n",
        "train(model_cnn_1, train_loader, n_epochs, criterion_cnn_1, optimizer_cnn_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15JaAob3Sn0q",
        "outputId": "d84b379c-4d14-4e3e-ccea-5c96224764bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1. Train Loss: 48.598\n",
            "Epoch: 2. Train Loss: 8.707\n",
            "Epoch: 3. Train Loss: 5.543\n",
            "Epoch: 4. Train Loss: 4.554\n",
            "Epoch: 5. Train Loss: 3.726\n",
            "Epoch: 6. Train Loss: 3.678\n",
            "Epoch: 7. Train Loss: 2.690\n",
            "Epoch: 8. Train Loss: 2.424\n",
            "Epoch: 9. Train Loss: 2.451\n",
            "Epoch: 10. Train Loss: 2.532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Red convolucional compleja con 4 bloques convolucionales\n",
        "train(model_cnn_2, train_loader, n_epochs, criterion_cnn_2, optimizer_cnn_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZoIGmNkdtWe",
        "outputId": "69b3beff-7114-4fa8-9825-e5322398f858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1. Train Loss: 89.986\n",
            "Epoch: 2. Train Loss: 6.057\n",
            "Epoch: 3. Train Loss: 4.387\n",
            "Epoch: 4. Train Loss: 3.033\n",
            "Epoch: 5. Train Loss: 2.782\n",
            "Epoch: 6. Train Loss: 3.902\n",
            "Epoch: 7. Train Loss: 1.992\n",
            "Epoch: 8. Train Loss: 1.762\n",
            "Epoch: 9. Train Loss: 1.347\n",
            "Epoch: 10. Train Loss: 1.121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar las redes\n",
        "acc_lineal = evaluate(test_loader, model_fc, criterion_fc)\n",
        "acc_cnn_1 = evaluate(test_loader, model_cnn_1, criterion_cnn_1)\n",
        "acc_cnn_2 = evaluate(test_loader, model_cnn_2, criterion_cnn_2)"
      ],
      "metadata": {
        "id": "BI9kUo7K2Pgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy red lineal: {round(acc_lineal, 4)}\")\n",
        "print(f\"Accuracy primera red convolucional: {round(acc_cnn_1, 4)}\")\n",
        "print(f\"Accuracy segunda red convolucional: {round(acc_cnn_2, 4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bksh_TrNyvTh",
        "outputId": "7202e301-b971-4154-b023-7dba109a3fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy red lineal: 0.9791\n",
            "Accuracy primera red convolucional: 0.9984\n",
            "Accuracy segunda red convolucional: 0.9989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esFjRe0xXafY"
      },
      "source": [
        "**Comente los resultados**\n",
        "\n",
        "* Respecto del entrenamiento, se observa que para los 3 modelos la p茅rdida va disminuyendo en cada 茅poca, siendo la diferencia entre una 茅poca y otra cada vez menor, por lo que la red est谩 aprendiendo convergiendo a un m铆nimo en la funci贸n de p茅rdida. De los 3 modelos definidos, el que logra una menor p茅rdida en entrenamiento(por ende, menor error) es la segunda red convolucional en la 茅poca 10. Destaca en todo caso la gran diferencia entre los valores de p茅rdida obtenidos mediante la red lineal y las redes convolucionales, siendo mejores las segundas. Esto tiene sentido, ya que las redes convolucionales intentan simular la forma en que la visi贸n procesa im谩genes, aplicando convoluciones, por lo que se espera que su desempe帽o sea mejor que el de una red lineal.\n",
        "\n",
        "* Respecto del accuracy en los datos de test, los tres modelos logran un muy buen desempe帽o, siendo levemente mejor en el caso de las redes convolucionales, de las cuales la que logra una leve mejora es la segunda red convolucional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EOw9BtRXafY"
      },
      "source": [
        "# Conclusi贸n\n",
        "\n",
        "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/vKSR-ZakVMIAAAAC/pochitadancing-pochita.gif\">\n",
        "</p>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "74a3d55773e9b3c246359d50654dd4558d2f2e3d7e6e6cc3c5c43d1d4b65ec31"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}